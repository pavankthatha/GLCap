{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#sys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 846013926886309887\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 841046164925501439\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 837350299513782272\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 831231734930890756\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 825274753673998335\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 821159870938411008\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 815067469689786367\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 808017015491465219\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 801198600478978047\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 794542503911272447\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 789126577116237827\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 783200081319112704\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 776926188325380096\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 770984564617465855\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 764078527545171967\n",
      "...3200 tweets downloaded so far\n",
      "getting tweets before 757251557800804352\n",
      "...3222 tweets downloaded so far\n",
      "getting tweets before 756217256929996800\n",
      "...3222 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import csv\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = \"eLTHNVGuK3e6ZDDENKJD9k6w8\"\n",
    "consumer_secret = \"UbJDVAj75efcWQ1mbVVrbFH10DbdewJhV74HPu9BfMm7Uafe6T\"\n",
    "access_key = \"118637669-Xce1sDf9KRVlBlarMCZJ9PX51mJJ2OImdVtVwtyl\"\n",
    "access_secret = \"r9D9yhxoJCai6clYfOcVEp1HQVdhxZD6wDQ9ZXeMwKcWd\"\n",
    "\n",
    "NoOfTweets=900 # May be assuming 5 per day; would get 6 months data\n",
    "\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=NoOfTweets)\n",
    "    \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print (\"getting tweets before %s\" % (oldest))\n",
    "        \n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=NoOfTweets,max_id=oldest)\n",
    "        \n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "        \n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        \n",
    "        print (\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\t\n",
    "    outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")] for tweet in alltweets]\n",
    "    \n",
    "    '''\n",
    "    #write the csv\t\n",
    "    with open('%s_tweets.csv' % screen_name, 'wb') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\"])\n",
    "        writer.writerows(outtweets)\n",
    "    \n",
    "    pass\n",
    "    '''\n",
    "    return outtweets\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #pass in the username of the account you want to download\n",
    "    outtweets = get_all_tweets(\"wipro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outtweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-adc228feb353>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;31m#writer = UnicodeWriter(f)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"created_at\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouttweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'outtweets' is not defined"
     ]
    }
   ],
   "source": [
    "import csv, codecs, cStringIO\n",
    "\n",
    "class UTF8Recoder:\n",
    "    \"\"\"\n",
    "    Iterator that reads an encoded stream and reencodes the input to UTF-8\n",
    "    \"\"\"\n",
    "    def __init__(self, f, encoding):\n",
    "        self.reader = codecs.getreader(encoding)(f)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        return self.reader.next().encode(\"utf-8\")\n",
    "\n",
    "class UnicodeReader:\n",
    "    \"\"\"\n",
    "    A CSV reader which will iterate over lines in the CSV file \"f\",\n",
    "    which is encoded in the given encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f, dialect=csv.excel, encoding=\"utf-8\", **kwds):\n",
    "        f = UTF8Recoder(f, encoding)\n",
    "        self.reader = csv.reader(f, dialect=dialect, **kwds)\n",
    "\n",
    "    def next(self):\n",
    "        row = self.reader.next()\n",
    "        return [unicode(s, \"utf-8\") for s in row]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "class UnicodeWriter:\n",
    "    \"\"\"\n",
    "    A CSV writer which will write rows to CSV file \"f\",\n",
    "    which is encoded in the given encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, f, dialect=csv.excel, encoding=\"utf-8\", **kwds):\n",
    "        # Redirect output to a queue\n",
    "        self.queue = cStringIO.StringIO()\n",
    "        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)\n",
    "        self.stream = f\n",
    "        self.encoder = codecs.getincrementalencoder(encoding)()\n",
    "\n",
    "    def writerow(self, row):\n",
    "        self.writer.writerow([s.encode(\"utf-8\") for s in row])\n",
    "        # Fetch UTF-8 output from the queue ...\n",
    "        data = self.queue.getvalue()\n",
    "        data = data.decode(\"utf-8\")\n",
    "        # ... and reencode it into the target encoding\n",
    "        data = self.encoder.encode(data)\n",
    "        # write to the target stream\n",
    "        self.stream.write(data)\n",
    "        # empty queue\n",
    "        self.queue.truncate(0)\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "with open('wipro_tweets.csv', 'wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    #writer = UnicodeWriter(f)\n",
    "    writer.writerow([\"id\",\"created_at\",\"text\"])\n",
    "    writer.writerows(outtweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wiproTweets = pd.read_csv('wipro_tweets_hash.csv')\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "'''\n",
    "myre = re.compile(u'['\n",
    "                  u'\\U0001F300-\\U0001F64F'\n",
    "                  u'\\U0001F680-\\U0001F6FF'\n",
    "                  u'\\u2600-\\u26FF\\u2700-\\u27BF]+', \n",
    "                  re.UNICODE)\n",
    "\n",
    "'''\n",
    "#i = myre.sub('', u'Some example text with a sleepy face: \\U0001f62a')\n",
    " \n",
    "def isNaN(num):\n",
    "    return num != num\n",
    "#text.replaceAll(\"[^\\\\u009d]\", \"\\uFFFD\");\n",
    "#for index,i in enumerate(wiproTweets['hashtags']):\n",
    "    #i = myre.replaceAll('', i)\n",
    "#    if (isNaN(i)):\n",
    "#        i = \" \"\n",
    "#    print (index,i)\n",
    "#    print (i.decode(\"utf-8\").strip())\n",
    "\n",
    "def handleNan(num):\n",
    "    if (isNaN(num)):\n",
    "        return \"\"\n",
    "    else:\n",
    "        return num\n",
    "wiproTweets['hashtags'] = wiproTweets['hashtags'].apply(lambda x: handleNan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wiproTweets\n",
    "def getwordStartingWithpatternInSentence(sentence, pattern):\n",
    "    return \" \".join([word for word in sentence.split() if word.startswith(pattern)])\n",
    "\n",
    "\n",
    "#print (getwordStartingWithpatternInSentence(\"Hey Sahil #Wipro is great #company\"\n",
    "#      ,'#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wiproTweets\n",
    "#wiproTweets['hashtags'] = wiproTweets['text'].apply(lambda x: getwordStartingWithpatternInSentence(x, '#'))\n",
    "\n",
    "#wiproTweets.to_csv(\"wipro_tweets_hash.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "wiproTweets['text'] = wiproTweets['text'].map(lambda words: re.sub(r'\\w*\\d\\w*', '', str(words)).strip())\n",
    "wiproTweets['text'] = wiproTweets['text'].map(lambda words: re.sub(r'http://.', '', str(words)).strip())\n",
    "wiproTweets['text'] = wiproTweets['text'].map(lambda words: re.sub(r'https://.', '', str(words)).strip())\n",
    "wiproTweets['text'] = wiproTweets['text'].map(lambda words: re.sub(r'â€¦', '', str(words)).strip())\n",
    "wiproTweets['text'] = wiproTweets['text'].map(lambda words: re.sub(r'co/', '', str(words)).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wiproTweets['text'] = wiproTweets['text'].apply(lambda x: x.decode('utf-8').strip())\n",
    "\n",
    "#for i in wiproTweets['text']:\n",
    "#    print (i)\n",
    "#    print (i.decode(\"utf-8\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "wiproTweets = wiproTweets.iloc[0:2630]\n",
    "wiproTweetsFiltered = wiproTweets[wiproTweets['hashtags'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "cntvectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.077s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.044s.\n",
      "Fitting the NMF model with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.446s.\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(wiproTweetsFiltered['hashtags'])\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(wiproTweetsFiltered['hashtags'])\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.getmaxprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "for t in tf[1]:\n",
    "    print (t.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import sys\n",
    " \n",
    "# maps words to their counts\n",
    "word2count = {}\n",
    " \n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "        word2count[word] = word2count.get(word, 0) + count\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        pass\n",
    " \n",
    "# sort the words lexigraphically;\n",
    "# this step is NOT required, we just do it so that our\n",
    "# final output will look more like the official Hadoop\n",
    "# word count examples\n",
    "sorted_word2count = sorted(word2count.items(), key=itemgetter(0))\n",
    " \n",
    "# write the results to STDOUT (standard output)\n",
    "for word, count in sorted_word2count:\n",
    "    print ('%s\\t%s'% (word, count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2385, 587)\n",
      "(587L,) (587L,)\n",
      "\n",
      "(4159L,)\n",
      "587\n"
     ]
    }
   ],
   "source": [
    "#print(\"\\nTopics in NMF model:\")\n",
    "#tfidf_feature_names = cntvectorizer.get_feature_names()\n",
    "#print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "import numpy as np\n",
    "\n",
    "print (tf.shape)\n",
    "print (tf.A[0,:].shape,tf.A[1,:].shape)\n",
    "print()\n",
    "\n",
    "print(tf.indices.shape)\n",
    "print (len(set(tf.indices)))\n",
    "\n",
    "word_count = {}\n",
    "tfidf_count = {}\n",
    "for index,str in enumerate(tf_vectorizer.get_feature_names()):\n",
    "    #word_count+={str:np.sum(tf.A[:,index])}\n",
    "    word_count[str] = np.sum(tf.A[:,index])\n",
    "    \n",
    "for index,str in enumerate(tfidf_vectorizer.get_feature_names()):\n",
    "    #word_count+={str:np.sum(tf.A[:,index])}\n",
    "    tfidf_count[str] = np.sum(tfidf.A[:,index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "#x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
    "sorted_x = sorted(word_count.items(), key=operator.itemgetter(1))\n",
    "sorted_hash = sorted(tfidf_count.items(), key=operator.itemgetter(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'promax', 3.3879331711916825),\n",
       " (u'wiproeot', 3.4268163893709307),\n",
       " (u'tco', 3.4357972302303792),\n",
       " (u'automated', 3.4402889826657232),\n",
       " (u'mobilefirst', 3.4477857390886841),\n",
       " (u'rpa', 3.4641174786324811),\n",
       " (u'managedservices', 3.475402238106271),\n",
       " (u'artificialintelligence', 3.5006171788458471),\n",
       " (u'businesstransformation', 3.5349316602645997),\n",
       " (u'connecteddevices', 3.5357340867354523),\n",
       " (u'dataanalytics', 3.5446520727374637),\n",
       " (u'gartnersec', 3.5567148516808),\n",
       " (u'turnkey', 3.5761027950919444),\n",
       " (u'mobileindustry', 3.5886970407244316),\n",
       " (u'cognitivecomputing', 3.5958793863702914),\n",
       " (u'cloudworld', 3.6084749067919852),\n",
       " (u'student', 3.6221628394433845),\n",
       " (u'retailers', 3.625054209568459),\n",
       " (u'ittransformation', 3.646207447805395),\n",
       " (u'datamigration', 3.6716488036805783),\n",
       " (u'digitalera', 3.67730159201595),\n",
       " (u'customertestimonial', 3.6877884633117683),\n",
       " (u'customersatisfaction', 3.7010002942774918),\n",
       " (u'engineering', 3.701831306476385),\n",
       " (u'demonetization', 3.7275119610034526),\n",
       " (u'riskmanagement', 3.7489337513539382),\n",
       " (u'ml', 3.7724812448144123),\n",
       " (u'design', 3.7879942111818847),\n",
       " (u'phm', 3.8066986329583208),\n",
       " (u'telecomoperators', 3.8801437460017629),\n",
       " (u'digitalstrategy', 3.8830399441398802),\n",
       " (u'sdn', 3.90517741821481),\n",
       " (u'vulnerabilityassessment', 3.9116653004063),\n",
       " (u'digitalserviceprovider', 3.9116997601887249),\n",
       " (u'kyc', 3.940829681681012),\n",
       " (u'magicquadrant', 3.9511821642631673),\n",
       " (u'digitalage', 3.9598586770237296),\n",
       " (u'enterprisemobility', 3.9839624371618485),\n",
       " (u'collaborationpartner', 4.0),\n",
       " (u'utilities', 4.0260230165438937),\n",
       " (u'nsb', 4.060673486985749),\n",
       " (u'culture', 4.1149427658908948),\n",
       " (u'government', 4.1181963454943542),\n",
       " (u'cloudinfrastructure', 4.1201392105999055),\n",
       " (u'security', 4.1266847284541246),\n",
       " (u'integratedoperations', 4.2038577308821132),\n",
       " (u'sowrun', 4.2215614850170091),\n",
       " (u'africacom', 4.2349693854312687),\n",
       " (u'enterpriseintegration', 4.2660318214772142),\n",
       " (u'frauddetection', 4.2753030598275243),\n",
       " (u'doddfrankact', 4.3155942806174492),\n",
       " (u'insurancecarriers', 4.3458647045527963),\n",
       " (u'banking', 4.393493476483572),\n",
       " (u'apps', 4.4101267821587795),\n",
       " (u'bank', 4.4180797395290279),\n",
       " (u'communicationservice', 4.467267474893827),\n",
       " (u'serviceproviders', 4.5410077745993132),\n",
       " (u'oraclembx', 4.5774291983937161),\n",
       " (u'digitization', 4.6240822138237236),\n",
       " (u'digitaldisruption', 4.668947018860294),\n",
       " (u'customerengagement', 4.6702102806198154),\n",
       " (u'startups', 4.6777480718202309),\n",
       " (u'datamanagement', 4.683269465362196),\n",
       " (u'hpediscover', 4.7548569169696204),\n",
       " (u'wipro', 4.7771440385226214),\n",
       " (u'cdsusa', 4.8103259204429989),\n",
       " (u'winclusive', 4.8253758783027809),\n",
       " (u'iheartcloud', 4.8378306300073861),\n",
       " (u'agility', 4.8588948320087262),\n",
       " (u'virtual', 4.8702658127618736),\n",
       " (u'mobilestrategy', 4.889874890699855),\n",
       " (u'chatbots', 4.8985767172903198),\n",
       " (u'wiproceo', 4.9023295217566929),\n",
       " (u'turnkeyprojects', 4.9669489087249019),\n",
       " (u'mobileapps', 4.9763328752669036),\n",
       " (u'mobileintelligent', 5.0),\n",
       " (u'studentrelationship', 5.0192810745451313),\n",
       " (u'leadership', 5.0285430890925102),\n",
       " (u'oems', 5.0460887616858052),\n",
       " (u'cio', 5.1285247893305455),\n",
       " (u'datascience', 5.1471743901030296),\n",
       " (u'googletest', 5.1664719513030235),\n",
       " (u'cloudassurance', 5.1826362404338777),\n",
       " (u'ibmamplify', 5.1878613019543272),\n",
       " (u'cloudpoweredenterprise', 5.2597566916945206),\n",
       " (u'datascientists', 5.3134754284909222),\n",
       " (u'iatawcs', 5.3235837540230877),\n",
       " (u'bots', 5.414612636410487),\n",
       " (u'susecon', 5.4156495179899569),\n",
       " (u'asaservice', 5.4415619233251142),\n",
       " (u'corporatetravel', 5.5972088788869749),\n",
       " (u'ui', 5.648975510908639),\n",
       " (u'edtech', 5.7651933173338223),\n",
       " (u'businessecosystem', 5.8414190471340461),\n",
       " (u'southafrica', 5.9312970774022595),\n",
       " (u'wipropromax', 5.9865976821721389),\n",
       " (u'disruption', 6.004570918609442),\n",
       " (u'bitcoin', 6.0267559075264829),\n",
       " (u'emm', 6.0791568308918214),\n",
       " (u'webinar', 6.081379042025624),\n",
       " (u'banks', 6.1406842891156508),\n",
       " (u'customerexperience', 6.152345557141274),\n",
       " (u'differentlyabled', 6.2428199899028574),\n",
       " (u'plm', 6.3146748782288418),\n",
       " (u'smartcities', 6.4324513822598259),\n",
       " (u'itservices', 6.4837995405862108),\n",
       " (u'technology', 6.4901671839983184),\n",
       " (u'digitalecosystem', 6.5194032092530092),\n",
       " (u'telcos', 6.556157073352006),\n",
       " (u'cloudmigration', 6.5600866398021012),\n",
       " (u'digitalmarketing', 6.5931918510892267),\n",
       " (u'datacenter', 6.5955133284948495),\n",
       " (u'datainsights', 6.6076531424419942),\n",
       " (u'virtualization', 6.6076622155262186),\n",
       " (u'crowdsourcing', 6.6573374391144204),\n",
       " (u'fiduciaryrule', 6.6969411120071616),\n",
       " (u'intelligentautomation', 6.7948101443670339),\n",
       " (u'cpa', 6.8054121611785376),\n",
       " (u'mainframes', 6.8073474805934193),\n",
       " (u'datadiscovery', 6.9204969917383714),\n",
       " (u'capitalmarkets', 6.9240805417393245),\n",
       " (u'tradepromotions', 6.9938335037072124),\n",
       " (u'bldc', 7.0245912102367711),\n",
       " (u'cios', 7.0760420842590523),\n",
       " (u'designthinking', 7.207446019175479),\n",
       " (u'revenue', 7.345389354233844),\n",
       " (u'supplychain', 7.4740299627020761),\n",
       " (u'iiot', 7.5329572266364595),\n",
       " (u'itinfrastructure', 7.6216853539444767),\n",
       " (u'wiproawsreinvent', 7.8237458736576819),\n",
       " (u'mobileapp', 7.8410701365349365),\n",
       " (u'tweetchat', 7.9374826986146898),\n",
       " (u'bpo', 8.0174440520640449),\n",
       " (u'anomalydetection', 8.0569266432193896),\n",
       " (u'lifesciences', 8.0797001371281905),\n",
       " (u'datavirtualization', 8.0820161720682702),\n",
       " (u'sdx', 8.2696523807756481),\n",
       " (u'testing', 8.3059972320105082),\n",
       " (u'cognitive', 8.421567713313669),\n",
       " (u'cloudcomputing', 8.5330708951662615),\n",
       " (u'analyst', 8.5428879963750184),\n",
       " (u'oem', 8.5510441478152579),\n",
       " (u'enterprisearchitecture', 8.6001230328835128),\n",
       " (u'tech', 8.6580307994688237),\n",
       " (u'nfv', 8.9743557054625622),\n",
       " (u'roi', 9.3066132468295528),\n",
       " (u'poisummit', 9.3327009548916013),\n",
       " (u'auto', 9.4222159369629868),\n",
       " (u'mobile', 9.4399662165600855),\n",
       " (u'automotive', 9.663710987750024),\n",
       " (u'oilandgas', 9.7685960892070263),\n",
       " (u'wearables', 10.229380002943213),\n",
       " (u'adobesummit', 10.509081171789848),\n",
       " (u'bethenew', 10.521478653212373),\n",
       " (u'manufacturing', 10.549098747352124),\n",
       " (u'wiprosdxsummit', 10.74768709551152),\n",
       " (u'ekyc', 10.772109580421905),\n",
       " (u'moments', 11.090647055914678),\n",
       " (u'advancedanalytics', 11.309816936090488),\n",
       " (u'wiprooow', 11.540440199224072),\n",
       " (u'sapphirenow', 11.694829838771751),\n",
       " (u'holmes', 11.763435675202611),\n",
       " (u'robotics', 12.010124445974741),\n",
       " (u'msignite', 12.903655684083471),\n",
       " (u'reconciliation', 12.976492106920325),\n",
       " (u'data', 13.020907434957365),\n",
       " (u'telecom', 13.195415551284182),\n",
       " (u'ux', 13.493072304522169),\n",
       " (u'travelandexpense', 13.699211331440097),\n",
       " (u'mining', 13.734416339176326),\n",
       " (u'ar', 13.84938694028757),\n",
       " (u'wiprocares', 14.593010089662235),\n",
       " (u'devops', 15.2312951248463),\n",
       " (u'qa', 15.604756395631185),\n",
       " (u'cybersecurity', 15.697732642628933),\n",
       " (u'sap', 15.800007088592452),\n",
       " (u'tradepromotion', 16.276250346924904),\n",
       " (u'smartapplications', 16.338478561443502),\n",
       " (u'innovation', 17.038759964519191),\n",
       " (u'vr', 17.307274694451429),\n",
       " (u'aiuncovered', 17.425312987120542),\n",
       " (u'wiproatmwc', 17.939634068688836),\n",
       " (u'agile', 17.970150300397712),\n",
       " (u'healthcare', 21.402768611112123),\n",
       " (u'automation', 26.038299109445688),\n",
       " (u'machinelearning', 27.813010359530509),\n",
       " (u'bigdata', 29.528218440668049),\n",
       " (u'retail', 29.616016865107596),\n",
       " (u'cloud', 35.26030657472171),\n",
       " (u'theholmesadvantage', 37.279253056034079),\n",
       " (u'analytics', 38.246558017099659),\n",
       " (u'digitaltransformation', 43.832173418121506),\n",
       " (u'blockchainatwipro', 53.202076440257414),\n",
       " (u'digital', 54.791501126035996),\n",
       " (u'cx', 63.528341066038593),\n",
       " (u'woolmagazine', 64.476056761454089),\n",
       " (u'iot', 70.003726490137979),\n",
       " (u'wiproholmes', 76.432471881494479),\n",
       " (u'blockchain', 81.932437081587551),\n",
       " (u'ai', 138.18870315559872)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_hash[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named wordcloud",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-a9c792917484>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Convert all the required text into a single string here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#and store them in word_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named wordcloud"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Convert all the required text into a single string here \n",
    "#and store them in word_string\n",
    "\n",
    "#you can specify fonts, stopwords, background color and other options\n",
    "\n",
    "wordcloud = WordCloud(font_path='C:/Users/Dikshit/Desktop/last/VLC/skins/fonts/FreeSans.ttf',\n",
    "                          stopwords=STOPWORDS,\n",
    "                          background_color='white',\n",
    "                          width=1200,\n",
    "                          height=1000\n",
    "                         ).generate(wiproTweetsFiltered['hashtags'])\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "#print_top_words(nmf, tfidf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wiproTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
